{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qkR0DOmsFgok"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Raffi Akhdilputra\\Documents\\GitHub\\resume-cv-analyzer\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, Any\n",
        "from getpass import getpass\n",
        "\n",
        "# LangChain + Google GenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Document processing\n",
        "import fitz  # PyMuPDF\n",
        "from docx import Document\n",
        "\n",
        "# ML models\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Embedder model from huggingface\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EPTYeXq3IGFt"
      },
      "outputs": [],
      "source": [
        "# For file upload in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y51FjEOpWs4e",
        "outputId": "fcd57a50-8ae9-40bb-b530-e67b8ca72c09"
      },
      "outputs": [],
      "source": [
        "# Global config\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "GOOGLE_API_KEY = getpass('ENTER GEMINI API KEY: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IIlNINoHarMt"
      },
      "outputs": [],
      "source": [
        "# Embedder\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMBED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rg3zhxJSFkT0"
      },
      "outputs": [],
      "source": [
        "def scrape_pdf_with_gemini(file_path: str, api_key: str) -> str:\n",
        "    \"\"\"Use Gemini to extract and structure text from PDF.\"\"\"\n",
        "    # configure API key\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "    # first extract text using PyMuPDF\n",
        "    doc = fitz.open(file_path)\n",
        "    extracted_text = \"\"\n",
        "    for page in doc:\n",
        "        extracted_text += page.get_text()\n",
        "    doc.close()\n",
        "\n",
        "    # Then use Gemini to clean and structure the text\n",
        "    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The following is raw text extracted from a PDF resume/CV document.\n",
        "    Please clean, organize, and structure this text properly.\n",
        "    Preserve all information but make it well-formatted and readable.\n",
        "    Include all sections like contact info, summary, experience, education, skills, etc.\n",
        "\n",
        "    Raw text:\n",
        "    {extracted_text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def scrape_docx_with_gemini(file_path: str, api_key: str) -> str:\n",
        "    \"\"\"Use Gemini to extract and structure text from DOCX.\"\"\"\n",
        "    # Configure API key\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "    # First extract text using python-docx\n",
        "    doc = Document(file_path)\n",
        "    extracted_text = \"\\n\\n\".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])\n",
        "\n",
        "    # Then use Gemini to clean and structure the text\n",
        "    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The following is raw text extracted from a DOCX resume/CV document.\n",
        "    Please clean, organize, and structure this text properly.\n",
        "    Preserve all information but make it well-formatted and readable.\n",
        "    Include all sections like contact info, summary, experience, education, skills, etc.\n",
        "\n",
        "    Raw text:\n",
        "    {extracted_text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GZOHP7maG8fH"
      },
      "outputs": [],
      "source": [
        "# TODO: REPLACE WITH ACTUAL BUILDED MODELS\n",
        "\n",
        "class ResumeModels:\n",
        "    \"\"\"Container for your trained models.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize placeholder models\n",
        "        self.role_pipeline = self._init_role_model()\n",
        "        self.acceptance_pipeline = self._init_acceptance_model()\n",
        "\n",
        "    def _init_role_model(self):\n",
        "        \"\"\"Initialize role prediction model (replace with your trained model).\"\"\"\n",
        "        role_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "        role_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "        pipeline = Pipeline([('tfidf', role_vectorizer), ('clf', role_clf)])\n",
        "\n",
        "        # Train on sample data (replace with your real model loading)\n",
        "        X_sample = [\n",
        "            \"Experienced backend engineer with python, django, rest apis\",\n",
        "            \"Junior frontend developer skilled in react, javascript, css\",\n",
        "            \"Data scientist with experience in sklearn, pandas, ml models\",\n",
        "        ]\n",
        "        y_sample = [\"Backend Engineer\", \"Frontend Engineer\", \"Data Scientist\"]\n",
        "        pipeline.fit(X_sample, y_sample)\n",
        "\n",
        "        return pipeline\n",
        "\n",
        "    def _init_acceptance_model(self):\n",
        "        \"\"\"Initialize acceptance prediction model (replace with your trained model).\"\"\"\n",
        "        accept_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "        accept_clf = DummyClassifier(strategy=\"stratified\")\n",
        "        pipeline = Pipeline([('tfidf', accept_vectorizer), ('clf', accept_clf)])\n",
        "\n",
        "        # Train on sample data (replace with your real model loading)\n",
        "        X_sample = [\n",
        "            \"Experienced backend engineer with python, django, rest apis\",\n",
        "            \"Junior frontend developer skilled in react, javascript, css\",\n",
        "            \"Data scientist with experience in sklearn, pandas, ml models\",\n",
        "        ]\n",
        "        y_sample = [1, 0, 1]\n",
        "        pipeline.fit(X_sample, y_sample)\n",
        "\n",
        "        return pipeline\n",
        "\n",
        "    def predict_role(self, resume_text: str) -> str:\n",
        "        \"\"\"Predict job role from resume text.\"\"\"\n",
        "        try:\n",
        "            pred = self.role_pipeline.predict([resume_text])[0]\n",
        "            return str(pred)\n",
        "        except Exception as e:\n",
        "            # Fallback heuristic\n",
        "            text_lower = resume_text.lower()\n",
        "            if \"data\" in text_lower or \"machine learning\" in text_lower:\n",
        "                return \"Data Scientist\"\n",
        "            elif \"react\" in text_lower or \"frontend\" in text_lower:\n",
        "                return \"Frontend Engineer\"\n",
        "            else:\n",
        "                return \"Backend Engineer\"\n",
        "\n",
        "    def predict_acceptance(self, resume_text: str) -> float:\n",
        "        \"\"\"Predict acceptance probability (0-1).\"\"\"\n",
        "        try:\n",
        "            prob = self.acceptance_pipeline.predict_proba([resume_text])[0]\n",
        "            if len(prob) == 2:\n",
        "                return float(prob[1])\n",
        "            return float(prob.max())\n",
        "        except Exception as e:\n",
        "            # Fallback heuristic\n",
        "            keywords = [\"experienced\", \"senior\", \"lead\", \"machine learning\", \"phd\", \"master\"]\n",
        "            score = sum([1 for k in keywords if k in resume_text.lower()]) / len(keywords)\n",
        "            return float(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_ncLkHoNHCUR"
      },
      "outputs": [],
      "source": [
        "def compute_resume_jd_similarity(resume_text: str, jd_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compute similarity between resume and job description using embeddings.\n",
        "    Uses the embedder (all-MiniLM-L6-v2).\n",
        "    \"\"\"\n",
        "    # Get embeddings from your separate embedder module\n",
        "    texts = [resume_text, jd_text]\n",
        "    embeddings = embedder.encode(texts, show_progress_bar=False)\n",
        "\n",
        "    resume_vec = embeddings[0]\n",
        "    jd_vec = embeddings[1]\n",
        "\n",
        "    # Cosine similarity\n",
        "    cosine_sim = np.dot(resume_vec, jd_vec) / (\n",
        "        np.linalg.norm(resume_vec) * np.linalg.norm(jd_vec) + 1e-9\n",
        "    )\n",
        "\n",
        "    # Find missing keywords\n",
        "    jd_tokens = set(jd_text.lower().split())\n",
        "    resume_tokens = set(resume_text.lower().split())\n",
        "    missing_keywords = list(jd_tokens - resume_tokens)[:20]\n",
        "\n",
        "    return {\n",
        "        'similarity_score': float(cosine_sim),\n",
        "        'missing_keywords': missing_keywords\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lSzIJzXKHI8t"
      },
      "outputs": [],
      "source": [
        "def get_gemini_assessment(\n",
        "    resume_text: str,\n",
        "    jd_text: str,\n",
        "    predicted_role: str,\n",
        "    acceptance_prob: float,\n",
        "    similarity_score: float,\n",
        "    missing_keywords: list,\n",
        "    api_key: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Send all model outputs to Gemini for final scoring and revision recommendations.\n",
        "    \"\"\"\n",
        "    # Configure API key\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert resume/CV reviewer and career consultant.\n",
        "\n",
        "I have analyzed a resume using ML models and need your expert assessment and recommendations.\n",
        "\n",
        "**Resume Text:**\n",
        "{resume_text[:3000]}\n",
        "\n",
        "**Job Description:**\n",
        "{jd_text}\n",
        "\n",
        "**ML Model Analysis Results:**\n",
        "- Predicted Role: {predicted_role}\n",
        "- Acceptance Probability: {acceptance_prob:.2%}\n",
        "- Resume-JD Similarity Score: {similarity_score:.3f} (0-1 scale, based on semantic embeddings)\n",
        "- Missing Keywords from JD: {', '.join(missing_keywords[:15])}\n",
        "\n",
        "**Your Task:**\n",
        "Based on the resume, job description, and ML model outputs, provide:\n",
        "\n",
        "1. **Overall Score (0-100)**: A numerical score indicating how well this resume matches the job\n",
        "2. **Score Explanation**: 2-3 sentences explaining the score\n",
        "3. **Strengths**: 2-3 key strengths of this resume for this role\n",
        "4. **Weaknesses**: 2-3 key weaknesses or gaps\n",
        "5. **Revision Suggestions**: 5-7 specific, actionable suggestions to improve the resume (be concrete)\n",
        "6. **Keyword Recommendations**: Specific keywords/skills to add based on missing keywords\n",
        "7. **Summary Rewrite**: A suggested 2-3 sentence professional summary optimized for this role\n",
        "8. **ATS Optimization Tips**: 2-3 tips to improve ATS (Applicant Tracking System) compatibility\n",
        "\n",
        "Return your response in valid JSON format with these exact keys:\n",
        "{{\n",
        "  \"score\": <number 0-100>,\n",
        "  \"explanation\": \"<string>\",\n",
        "  \"strengths\": [\"<string>\", \"<string>\", ...],\n",
        "  \"weaknesses\": [\"<string>\", \"<string>\", ...],\n",
        "  \"revision_suggestions\": [\"<string>\", \"<string>\", ...],\n",
        "  \"keyword_recommendations\": [\"<string>\", \"<string>\", ...],\n",
        "  \"summary_rewrite\": \"<string>\",\n",
        "  \"ats_tips\": [\"<string>\", \"<string>\", ...]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract JSON from response\n",
        "        response_text = response.content.strip()\n",
        "        # Remove markdown code blocks if present\n",
        "        if response_text.startswith('```'):\n",
        "            response_text = response_text.split('```')[1]\n",
        "            if response_text.startswith('json'):\n",
        "                response_text = response_text[4:]\n",
        "            response_text = response_text.rsplit('```', 1)[0]\n",
        "\n",
        "        assessment = json.loads(response_text)\n",
        "        return assessment\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if JSON parsing fails\n",
        "        return {\n",
        "            'score': 0,\n",
        "            'explanation': 'Error parsing Gemini response',\n",
        "            'raw_response': response.content\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ISjX-MZvHWlQ"
      },
      "outputs": [],
      "source": [
        "def analyze_resume(\n",
        "    resume_file_path: str,\n",
        "    job_description: str,\n",
        "    api_key: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Complete resume analysis pipeline.\n",
        "\n",
        "    Steps:\n",
        "    1. Gemini scrapes the resume file\n",
        "    2. Your models analyze the resume\n",
        "    3. Embedder computes similarity\n",
        "    4. Gemini provides final assessment and recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Step 1: Extracting text from resume using Gemini...\")\n",
        "    # Scrape resume with Gemini\n",
        "    if resume_file_path.lower().endswith('.pdf'):\n",
        "        resume_text = scrape_pdf_with_gemini(resume_file_path, api_key)\n",
        "    elif resume_file_path.lower().endswith('.docx'):\n",
        "        resume_text = scrape_docx_with_gemini(resume_file_path, api_key)\n",
        "    else:\n",
        "        raise ValueError(\"File must be PDF or DOCX\")\n",
        "\n",
        "    print(\"Step 2: Running your ML models...\")\n",
        "    # Initialize your models\n",
        "    models = ResumeModels()\n",
        "\n",
        "    # Get predictions from your models\n",
        "    predicted_role = models.predict_role(resume_text)\n",
        "    acceptance_prob = models.predict_acceptance(resume_text)\n",
        "\n",
        "    print(\"Step 3: Computing embedding similarity...\")\n",
        "    # Compute similarity using your embedder\n",
        "    similarity_result = compute_resume_jd_similarity(resume_text, job_description)\n",
        "\n",
        "    print(\"Step 4: Getting Gemini assessment and recommendations...\")\n",
        "    # Get final assessment from Gemini\n",
        "    gemini_assessment = get_gemini_assessment(\n",
        "        resume_text=resume_text,\n",
        "        jd_text=job_description,\n",
        "        predicted_role=predicted_role,\n",
        "        acceptance_prob=acceptance_prob,\n",
        "        similarity_score=similarity_result['similarity_score'],\n",
        "        missing_keywords=similarity_result['missing_keywords'],\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    # Compile final results\n",
        "    results = {\n",
        "        'resume_text': resume_text,\n",
        "        'model_outputs': {\n",
        "            'predicted_role': predicted_role,\n",
        "            'acceptance_probability': acceptance_prob,\n",
        "            'similarity_score': similarity_result['similarity_score'],\n",
        "            'missing_keywords': similarity_result['missing_keywords']\n",
        "        },\n",
        "        'gemini_assessment': gemini_assessment\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IzwGLdI2HXK0"
      },
      "outputs": [],
      "source": [
        "# Example Job Description\n",
        "EXAMPLE_JD = \"\"\"\n",
        "We are seeking a Backend Engineer with 3+ years of experience in Python, Django, REST APIs, and\n",
        "cloud deployment (GCP or AWS). Experience with PostgreSQL and Docker is required. The candidate\n",
        "will build scalable microservices and own features end-to-end.\n",
        "\n",
        "Required Skills:\n",
        "- Python, Django, Flask\n",
        "- REST API design\n",
        "- PostgreSQL, Redis\n",
        "- Docker, Kubernetes\n",
        "- GCP or AWS\n",
        "- Git, CI/CD\n",
        "\n",
        "Nice to have:\n",
        "- Experience with microservices architecture\n",
        "- Knowledge of GraphQL\n",
        "- Frontend experience (React/Vue)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "HSqqlK0VHbgi",
        "outputId": "5f05f996-acf4-4ab6-a22c-6a8dfde9fcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not in Colab. Example usage:\n",
            "\n",
            "    # Set your API key\n",
            "    API_KEY = 'your-google-api-key'\n",
            "\n",
            "    # Analyze resume\n",
            "    results = analyze_resume('path/to/resume.pdf', EXAMPLE_JD, API_KEY)\n",
            "\n",
            "    # Print results\n",
            "    print(json.dumps(results['gemini_assessment'], indent=2))\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Upload and analyze\n",
        "if IN_COLAB:\n",
        "    print(\"Upload your resume (PDF or DOCX):\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        resume_file = list(uploaded.keys())[0]\n",
        "        print(f\"\\nAnalyzing: {resume_file}\\n\")\n",
        "\n",
        "        # Set your API key\n",
        "        API_KEY = input(\"Enter your Google API Key: \")\n",
        "\n",
        "        # Run analysis\n",
        "        results = analyze_resume(resume_file, EXAMPLE_JD, API_KEY)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RESUME ANALYSIS RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(\"\\n--- MODEL OUTPUTS ---\")\n",
        "        print(f\"Predicted Role: {results['model_outputs']['predicted_role']}\")\n",
        "        print(f\"Acceptance Probability: {results['model_outputs']['acceptance_probability']:.2%}\")\n",
        "        print(f\"Similarity Score: {results['model_outputs']['similarity_score']:.3f}\")\n",
        "        print(f\"Missing Keywords: {', '.join(results['model_outputs']['missing_keywords'][:10])}\")\n",
        "\n",
        "        print(\"\\n--- GEMINI ASSESSMENT ---\")\n",
        "        assessment = results['gemini_assessment']\n",
        "        print(f\"Overall Score: {assessment.get('score', 'N/A')}/100\")\n",
        "        print(f\"\\nExplanation: {assessment.get('explanation', 'N/A')}\")\n",
        "\n",
        "        if 'revision_suggestions' in assessment:\n",
        "            print(\"\\nRevision Suggestions:\")\n",
        "            for i, suggestion in enumerate(assessment['revision_suggestions'], 1):\n",
        "                print(f\"  {i}. {suggestion}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"\\nFull results saved in 'results' variable\")\n",
        "\n",
        "else:\n",
        "    print(\"Not in Colab. Example usage:\")\n",
        "    print(\"\"\"\n",
        "    # Set your API key\n",
        "    GOOGLE_API_KEY = 'your-google-api-key'\n",
        "\n",
        "    # Analyze resume\n",
        "    results = analyze_resume('path/to/resume.pdf', EXAMPLE_JD, API_KEY)\n",
        "\n",
        "    # Print results\n",
        "    print(json.dumps(results['gemini_assessment'], indent=2))\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7F11nc8Hp5W"
      },
      "outputs": [],
      "source": [
        "class ResumeAnalysisDebugger:\n",
        "    \"\"\"Comprehensive debugging tools for the resume analysis pipeline.\"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True):\n",
        "        self.verbose = verbose\n",
        "        self.logs = []\n",
        "\n",
        "    def log(self, message, level=\"INFO\"):\n",
        "        \"\"\"Log a message with timestamp.\"\"\"\n",
        "        import datetime\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
        "        self.logs.append(log_entry)\n",
        "        if self.verbose:\n",
        "            print(log_entry)\n",
        "\n",
        "    def test_gemini_connection(self, api_key: str) -> bool:\n",
        "        \"\"\"Test if Gemini API is accessible and working.\"\"\"\n",
        "        self.log(\"Testing Gemini API connection...\")\n",
        "        try:\n",
        "            os.environ['GOOGLE_API_KEY'] = api_key\n",
        "            llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.0)\n",
        "            response = llm.invoke(\"Say 'connection successful'\")\n",
        "            if response.content:\n",
        "                self.log(\"Gemini API connection successful\", \"SUCCESS\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            self.log(f\"Gemini API connection failed: {str(e)}\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "    def test_embedder_module(self) -> bool:\n",
        "        \"\"\"Test if embedder module is properly loaded.\"\"\"\n",
        "        self.log(\"Testing embedder module...\")\n",
        "        try:\n",
        "            test_text = [\"Hello world\", \"Test sentence\"]\n",
        "            embeddings = embedder.encode(test_text, show_progress_bar=False)\n",
        "            if embeddings.shape[0] == 2:\n",
        "                self.log(f\"Embedder loaded successfully. Embedding dimension: {embeddings.shape[1]}\", \"SUCCESS\")\n",
        "                return True\n",
        "        except ImportError as e:\n",
        "            self.log(f\"Cannot import embedder_module: {str(e)}\", \"ERROR\")\n",
        "            self.log(\"Make sure embedder_module.py exists in the same directory\", \"ERROR\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            self.log(f\"Embedder error: {str(e)}\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "    def test_file_scraping(self, file_path: str, api_key: str) -> Dict[str, Any]:\n",
        "        \"\"\"Test file scraping functionality.\"\"\"\n",
        "        self.log(f\"Testing file scraping for: {file_path}\")\n",
        "        results = {\n",
        "            'file_exists': False,\n",
        "            'file_type': None,\n",
        "            'scraping_successful': False,\n",
        "            'text_length': 0,\n",
        "            'error': None\n",
        "        }\n",
        "\n",
        "        # Check file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            results['error'] = \"File does not exist\"\n",
        "            self.log(f\"File not found: {file_path}\", \"ERROR\")\n",
        "            return results\n",
        "\n",
        "        results['file_exists'] = True\n",
        "\n",
        "        # Determine file type\n",
        "        if file_path.lower().endswith('.pdf'):\n",
        "            results['file_type'] = 'PDF'\n",
        "        elif file_path.lower().endswith('.docx'):\n",
        "            results['file_type'] = 'DOCX'\n",
        "        else:\n",
        "            results['error'] = \"Unsupported file type\"\n",
        "            self.log(\"File must be PDF or DOCX\", \"ERROR\")\n",
        "            return results\n",
        "\n",
        "        # Test scraping\n",
        "        try:\n",
        "            if results['file_type'] == 'PDF':\n",
        "                text = scrape_pdf_with_gemini(file_path, api_key)\n",
        "            else:\n",
        "                text = scrape_docx_with_gemini(file_path, api_key)\n",
        "\n",
        "            results['scraping_successful'] = True\n",
        "            results['text_length'] = len(text)\n",
        "            self.log(f\"Successfully scraped {results['text_length']} characters\", \"SUCCESS\")\n",
        "            self.log(f\"Preview: {text[:200]}...\", \"INFO\")\n",
        "\n",
        "        except Exception as e:\n",
        "            results['error'] = str(e)\n",
        "            self.log(f\"✗ Scraping failed: {str(e)}\", \"ERROR\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_models(self, sample_text: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Test if ML models are working.\"\"\"\n",
        "        self.log(\"Testing ML models...\")\n",
        "\n",
        "        if sample_text is None:\n",
        "            sample_text = \"\"\"\n",
        "            Experienced Software Engineer with 5 years in backend development.\n",
        "            Proficient in Python, Django, PostgreSQL, and AWS.\n",
        "            Led team of 3 developers in building scalable microservices.\n",
        "            \"\"\"\n",
        "\n",
        "        results = {\n",
        "            'role_prediction': None,\n",
        "            'acceptance_prediction': None,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            models = ResumeModels()\n",
        "\n",
        "            # Test role prediction\n",
        "            try:\n",
        "                role = models.predict_role(sample_text)\n",
        "                results['role_prediction'] = role\n",
        "                self.log(f\"Role prediction: {role}\", \"SUCCESS\")\n",
        "            except Exception as e:\n",
        "                results['errors'].append(f\"Role prediction error: {str(e)}\")\n",
        "                self.log(f\"✗ Role prediction failed: {str(e)}\", \"ERROR\")\n",
        "\n",
        "            # Test acceptance prediction\n",
        "            try:\n",
        "                acceptance = models.predict_acceptance(sample_text)\n",
        "                results['acceptance_prediction'] = acceptance\n",
        "                self.log(f\"Acceptance prediction: {acceptance:.2%}\", \"SUCCESS\")\n",
        "            except Exception as e:\n",
        "                results['errors'].append(f\"Acceptance prediction error: {str(e)}\")\n",
        "                self.log(f\"Acceptance prediction failed: {str(e)}\", \"ERROR\")\n",
        "\n",
        "        except Exception as e:\n",
        "            results['errors'].append(f\"Model initialization error: {str(e)}\")\n",
        "            self.log(f\"Model initialization failed: {str(e)}\", \"ERROR\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_similarity_computation(self) -> bool:\n",
        "        \"\"\"Test embedding similarity computation.\"\"\"\n",
        "        self.log(\"Testing similarity computation...\")\n",
        "\n",
        "        try:\n",
        "          resume = \"Python developer with Django experience\"\n",
        "          jd = \"Looking for Python Django developer\"\n",
        "\n",
        "          result = compute_resume_jd_similarity(resume, jd)\n",
        "\n",
        "          self.log(f\"Similarity score: {result['similarity_score']:.3f}\", \"SUCCESS\")\n",
        "          self.log(f\"Missing keywords: {result['missing_keywords'][:5]}\", \"INFO\")\n",
        "          return True\n",
        "\n",
        "        except Exception as e:\n",
        "          self.log(f\"Similarity computation failed: {str(e)}\", \"ERROR\")\n",
        "          return False\n",
        "\n",
        "    def test_gemini_assessment(self, api_key: str) -> bool:\n",
        "        \"\"\"Test Gemini assessment with sample data.\"\"\"\n",
        "        self.log(\"Testing Gemini assessment...\")\n",
        "\n",
        "        try:\n",
        "          # resume\n",
        "          sample_resume = \"Backend Engineer with Python, Django, PostgreSQL experience\"\n",
        "\n",
        "          # job description\n",
        "          sample_jd = \"Looking for Backend Engineer with Python skills\"\n",
        "\n",
        "          assessment = get_gemini_assessment(\n",
        "              resume_text=sample_resume,\n",
        "              jd_text=sample_jd,\n",
        "              predicted_role=\"Backend Engineer\",\n",
        "              acceptance_prob=0.75,\n",
        "              similarity_score=0.85,\n",
        "              missing_keywords=[\"docker\", \"kubernetes\"],\n",
        "              api_key=api_key\n",
        "          )\n",
        "\n",
        "          if 'score' in assessment:\n",
        "            self.log(f\"Gemini assessment successful. Score: {assessment['score']}/100\", \"SUCCESS\")\n",
        "            return True\n",
        "          else:\n",
        "            self.log(\"Gemini assessment returned incomplete response\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "          self.log(f\"Gemini assessment failed: {str(e)}\", \"ERROR\")\n",
        "          return False\n",
        "\n",
        "    def run_full_diagnostics(self, api_key: str, test_file_path: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"Run complete diagnostic tests on the entire pipeline.\"\"\"\n",
        "        self.log(\"=\"*60, \"INFO\")\n",
        "        self.log(\"STARTING FULL DIAGNOSTICS\", \"INFO\")\n",
        "        self.log(\"=\"*60, \"INFO\")\n",
        "\n",
        "        results = {\n",
        "            'gemini_connection': False,\n",
        "            'embedder_module': False,\n",
        "            'models': False,\n",
        "            'similarity': False,\n",
        "            'gemini_assessment': False,\n",
        "            'file_scraping': None,\n",
        "            'overall_status': 'FAILED'\n",
        "        }\n",
        "\n",
        "        # Test 1: Gemini connection\n",
        "        results['gemini_connection'] = self.test_gemini_connection(api_key)\n",
        "\n",
        "        # Test 2: Embedder module\n",
        "        results['embedder_module'] = self.test_embedder_module()\n",
        "\n",
        "        # Test 3: ML models\n",
        "        model_results = self.test_models()\n",
        "        results['models'] = len(model_results['errors']) == 0\n",
        "\n",
        "        # Test 4: Similarity computation\n",
        "        if results['embedder_module']:\n",
        "            results['similarity'] = self.test_similarity_computation()\n",
        "\n",
        "        # Test 5: Gemini assessment\n",
        "        if results['gemini_connection']:\n",
        "            results['gemini_assessment'] = self.test_gemini_assessment(api_key)\n",
        "\n",
        "        # Test 6: File scraping (optional)\n",
        "        if test_file_path and results['gemini_connection']:\n",
        "            results['file_scraping'] = self.test_file_scraping(test_file_path, api_key)\n",
        "\n",
        "        # Overall status\n",
        "        critical_tests = [\n",
        "            results['gemini_connection'],\n",
        "            results['embedder_module'],\n",
        "            results['models']\n",
        "        ]\n",
        "\n",
        "        if all(critical_tests):\n",
        "            results['overall_status'] = 'PASSED'\n",
        "            self.log(\"=\"*60, \"SUCCESS\")\n",
        "            self.log(\"ALL CRITICAL TESTS PASSED\", \"SUCCESS\")\n",
        "            self.log(\"=\"*60, \"SUCCESS\")\n",
        "        else:\n",
        "            self.log(\"=\"*60, \"ERROR\")\n",
        "            self.log(\"SOME TESTS FAILED - CHECK LOGS ABOVE\", \"ERROR\")\n",
        "            self.log(\"=\"*60, \"ERROR\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_logs(self, filename=\"debug_logs.txt\"):\n",
        "        \"\"\"Save all logs to a file.\"\"\"\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write('\\n'.join(self.logs))\n",
        "        self.log(f\"Logs saved to {filename}\", \"INFO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJWiQTaUSjXO",
        "outputId": "955f5691-6f4b-4078-b128-860a71eb838f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-01 23:00:47] [INFO] Testing Gemini API connection...\n",
            "[2025-12-01 23:00:48] [SUCCESS] Gemini API connection successful\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "debugger = ResumeAnalysisDebugger(verbose=True)\n",
        "\n",
        "debugger.test_gemini_connection(GOOGLE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LxMm4ImuTxQ2",
        "outputId": "5f207d79-edd5-4d2b-81b4-912f7ea0cd61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-01 23:00:50] [INFO] Testing file scraping for: /content/example.pdf\n",
            "[2025-12-01 23:00:50] [ERROR] File not found: /content/example.pdf\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'file_exists': False,\n",
              " 'file_type': None,\n",
              " 'scraping_successful': False,\n",
              " 'text_length': 0,\n",
              " 'error': 'File does not exist'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "debugger.test_file_scraping('', GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b63tXcVYU7Px",
        "outputId": "7cca0ad1-81ae-42b7-9913-1e75fc6ab04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-01 23:00:56] [INFO] Testing embedder module...\n",
            "[2025-12-01 23:00:56] [SUCCESS] Embedder loaded successfully. Embedding dimension: 384\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "debugger.test_embedder_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LOnFoUlHbOkA",
        "outputId": "3ee75688-f483-4d23-b83b-51622888b1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-01 15:07:13] [INFO] ============================================================\n",
            "[2025-12-01 15:07:13] [INFO] STARTING FULL DIAGNOSTICS\n",
            "[2025-12-01 15:07:13] [INFO] ============================================================\n",
            "[2025-12-01 15:07:13] [INFO] Testing Gemini API connection...\n",
            "[2025-12-01 15:07:14] [SUCCESS] Gemini API connection successful\n",
            "[2025-12-01 15:07:14] [INFO] Testing embedder module...\n",
            "[2025-12-01 15:07:14] [SUCCESS] Embedder loaded successfully. Embedding dimension: 384\n",
            "[2025-12-01 15:07:14] [INFO] Testing ML models...\n",
            "[2025-12-01 15:07:14] [SUCCESS] Role prediction: Backend Engineer\n",
            "[2025-12-01 15:07:14] [SUCCESS] Acceptance prediction: 100.00%\n",
            "[2025-12-01 15:07:14] [INFO] Testing similarity computation...\n",
            "[2025-12-01 15:07:14] [SUCCESS] Similarity score: 0.886\n",
            "[2025-12-01 15:07:14] [INFO] Missing keywords: ['looking', 'for']\n",
            "[2025-12-01 15:07:14] [INFO] Testing Gemini assessment...\n",
            "[2025-12-01 15:07:22] [SUCCESS] Gemini assessment successful. Score: 82/100\n",
            "[2025-12-01 15:07:22] [INFO] Testing file scraping for: /content/example.pdf\n",
            "[2025-12-01 15:07:36] [SUCCESS] Successfully scraped 5819 characters\n",
            "[2025-12-01 15:07:36] [INFO] Preview: Here's the cleaned, organized, and structured resume based on the raw text provided:\n",
            "\n",
            "---\n",
            "\n",
            "**MUHAMMAD RAFFI AKHDILPUTRA**\n",
            "Data Analyst | Data Science Enthusiast | Machine Learning Developer\n",
            "\n",
            "Jakarta S...\n",
            "[2025-12-01 15:07:36] [SUCCESS] ============================================================\n",
            "[2025-12-01 15:07:36] [SUCCESS] ALL CRITICAL TESTS PASSED\n",
            "[2025-12-01 15:07:36] [SUCCESS] ============================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'gemini_connection': True,\n",
              " 'embedder_module': True,\n",
              " 'models': True,\n",
              " 'similarity': True,\n",
              " 'gemini_assessment': True,\n",
              " 'file_scraping': {'file_exists': True,\n",
              "  'file_type': 'PDF',\n",
              "  'scraping_successful': True,\n",
              "  'text_length': 5819,\n",
              "  'error': None},\n",
              " 'overall_status': 'PASSED'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "debugger.run_full_diagnostics(GOOGLE_API_KEY, '/content/example.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKZI6YolcJmg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
